{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Importing required modules\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import time\n",
    "import scipy\n",
    "import sklearn\n",
    "\n",
    "import feedparser as fp\n",
    "import html\n",
    "from nltk.tokenize import TreebankWordTokenizer,WhitespaceTokenizer,word_tokenize\n",
    "from gensim.summarization import keywords\n",
    "import string\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "import re\n",
    "from langdetect import detect\n",
    "from website.code_bin import Cleaner\n",
    "from bs4 import BeautifulSoup\n",
    "from nltk.corpus import stopwords\n",
    "from multiprocessing import Pool\n",
    "import string\n",
    "from newspaper import Article\n",
    "\n",
    "## Models\n",
    "from gensim.models.doc2vec import Doc2Vec, TaggedDocument\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "\n",
    "# Plotting\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Scan output folder for pickle files w/ identifier and clean them. \n",
    "\n",
    "\n",
    "floc = '/Users/mohsenghassemi/Desktop/Job_Material/Insight/Project/summaries4/'\n",
    "foutloc = '/Users/mohsenghassemi/Desktop/Job_Material/Insight/Project/'\n",
    "identifier = '_1580326866'\n",
    "file_prefix = \"feeds_\"\n",
    "file_suffix = \".pkl\"\n",
    "\n",
    "#preprocessing wrapper function for multiprocessing\n",
    "def pp(summaries):\n",
    "    return (summaries[0],cleaner.preprocess_documents(summaries[1]))\n",
    "\n",
    "cleaner = Cleaner.Cleaner()\n",
    "\n",
    "nfeed_files = 205\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    p = Pool(8)\n",
    "    for i in range(nfeed_files):\n",
    "        cleaned_feeds = []\n",
    "        #load in file\n",
    "        fname = file_prefix + str(i) + identifier + file_suffix\n",
    "        with open(floc+fname,'rb') as fid:\n",
    "            thisfeeds = pickle.load(fid)\n",
    "        print(len(thisfeeds))\n",
    "        start_time = time.time()\n",
    "        cleaned_feeds = p.map(pp,thisfeeds)\n",
    "        print(len(cleaned_feeds))\n",
    "\n",
    "        with open(foutloc+'preprocessed_summaries'+str(i)+identifier+file_suffix,'wb') as fid:\n",
    "            pickle.dump(cleaned_feeds,fid)\n",
    "\n",
    "        stop_time = time.time()\n",
    "        duration = stop_time - start_time\n",
    "        print('Parsed file %s, duration = %.2f' % (fname,duration))\n",
    "        i += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Combine groups of 50 files, and combine preprocessed descriptions into strings. \n",
    "#Reduces the number of files and saves space for RAM efficiency. \n",
    "import time\n",
    "import pickle\n",
    "\n",
    "floc = '/Users/mohsenghassemi/Desktop/Job_Material/Insight/Project/summaries5/'\n",
    "foutloc = '/Users/mohsenghassemi/Desktop/Job_Material/Insight/Project/summaries6/'\n",
    "associator = '_1580538675'\n",
    "ext = '.pkl'\n",
    "\n",
    "file_chunk_size = 50\n",
    "nfile = 126\n",
    "fctr = 0\n",
    "for fr in range(0,nfile,file_chunk_size):\n",
    "    preprocessed_summaries = []\n",
    "    for i in range(fr,min(fr+file_chunk_size,nfile-1)):\n",
    "\n",
    "        start_time = time.time()\n",
    "        \n",
    "\n",
    "        \n",
    "        fname = 'preprocessed_summaries' + str(i) + associator + ext\n",
    "        with open(floc+fname,'rb') as fid:\n",
    "            thisdata = pickle.load(fid)\n",
    "        \n",
    "        for t in thisdata:\n",
    "            for j in range(0,len(t[1])):\n",
    "                t[1][j] = ' '.join(t[1][j])\n",
    "            preprocessed_summaries.append(t)\n",
    "        \n",
    "        stop_time = time.time()\n",
    "        duration = stop_time - start_time\n",
    "        print('loaded file %d, duration = %.2f' % (i,duration))\n",
    "    print('saving chunk %d...' % fr)\n",
    "    with open(foutloc + 'preprocessed_summaries' + str(fctr) + associator + ext,'wb') as fid:\n",
    "        pickle.dump(preprocessed_summaries,fid)\n",
    "        fctr += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading and Preparing Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a data frame where each row is a podcast and all episode descriptios for each podcasts is found in 'podcasts_eps_descriptions'. In this dataframe, stopwords and punctuations are removed, making it suitable for word2vec and doc2vec. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create dataset of podcasts\n",
    "import pickle\n",
    "import time\n",
    "\n",
    "floc = '/Users/mohsenghassemi/Desktop/Job_Material/Insight/Project/summaries/'\n",
    "fctr = 0\n",
    "associator = '1580326866'\n",
    "nfile = 4\n",
    "episodes = []\n",
    "just_podcast_vectors=[]\n",
    "for i in range(0,nfile):\n",
    "    start_time = time.time()\n",
    "    fname = 'preprocessed_summaries' + str(fctr) +'_'+ associator + '.pkl'\n",
    "    with open(floc+fname,'rb') as fid:\n",
    "         pps = pickle.load(fid)\n",
    "    for p in pps:\n",
    "        #just_podcast_vectors.append(pod2vec(p[1]))\n",
    "        episodes.append([p[0],p[1]])\n",
    "    fctr += 1\n",
    "    stop_time = time.time()\n",
    "    duration = stop_time - start_time\n",
    "    print('Done with file ' + str(i) + ' (duration=' + str(duration) + ')')\n",
    "\n",
    "podcasts_df=pd.DataFrame(episodes)\n",
    "podcasts_df.rename(columns = {0:'feedUrl',1:'podcasts_eps_descriptions'}, inplace = True)\n",
    "fname = 'popular_podcasts_preprocessed' +'_'+ associator + '.pkl'\n",
    "with open(floc+fname,'wb') as fid:\n",
    "         pickle.dump(podcasts_df,fid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparing data for GLoVE and Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Create dataset of episodes'''\n",
    "\n",
    "\n",
    "# First, let us load the podcasts dataframe we created in the last section.\n",
    "floc = '/Users/mohsenghassemi/Desktop/Job_Material/Insight/Project/summaries6/'\n",
    "associator = '1580326866'\n",
    "fname = 'popular_podcasts_preprocessed' +'_'+ associator + '.pkl'\n",
    "with open(floc+fname,'rb') as fid:\n",
    "         podcasts_df=pickle.load(fid)\n",
    "        \n",
    "# Next, parse and clean the episode descriptions to prepare them for BERT (keep stopwords and punctuations).\n",
    "podcasts_df['episodes_descriptions_unprocessed'] = podcasts_df.apply(lambda x:get_unprocessed_eps(x),axis=1)\n",
    "podcasts_df['episodes_descriptions_processed'] = podcasts_df['episodes_descriptions_unprocessed'].apply(lambda x:get_processed_eps(x))\n",
    "\n",
    "\n",
    "# Here, we create a dataframe where each row corresponds to one episode.\n",
    "temp=podcasts_df.merge(podcasts_df.episodes_descriptions_processed.apply(pd.Series), left_index = True, right_index = True)\n",
    "temp.drop(['episodes_descriptions_unprocesses','podcasts_eps_descriptions'], axis=1, inplace=True)\n",
    "episodes_df=temp.melt(id_vars = ['feedUrl', 'episodes_descriptions_processed'], value_name = 'episodes_descriptions')\n",
    "episodes_df.dropna(inplace=True)\n",
    "episodes_df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "with open('episodes_glove_w2v','wb') as fid:\n",
    "         pickle.dump(episodes_df,fid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparing data for Sentence BERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Some helper functions to process the texts'''\n",
    "\n",
    "def get_unprocessed_eps(row):\n",
    "    try:\n",
    "        a=fp.parse(row.feedUrl)\n",
    "        return [a['entries'][k]['content'][0]['value'] for k in range(0,len(a['entries']))]\n",
    "    except:\n",
    "        return None\n",
    "    \n",
    "    \n",
    "def remove_html_tags(text):\n",
    "    '''This function removes html tags from a string, e.g. <br></br>'''\n",
    "    soup = BeautifulSoup(text)\n",
    "    return soup.get_text()\n",
    "      \n",
    "\n",
    "def replace_newline_NBSP(text):\n",
    "    '''Replaces newline characters with white space'''\n",
    "    return text.replace('\\n',' ').replace('\\xa0',' ')\n",
    "\n",
    "\n",
    "def replace_dash(text,on=True):\n",
    "    '''replaces dash characters with white space. Can be turned off.'''\n",
    "    if(on):\n",
    "        return text.replace('-',' ')\n",
    "    else:\n",
    "        return text\n",
    "\n",
    "    \n",
    "def clean_BERT(text,rep_dash=True):\n",
    "    return ''.join(c for c in remove_html_tags(replace_dash(replace_newline_NBSP(html.unescape(text)),rep_dash)) if \n",
    "                  c not in ['\"',\"'\",'â€™','`'])\n",
    "\n",
    " \n",
    "def get_processed_eps(eps):\n",
    "    try:\n",
    "        return [clean_BERT(eps[k]) for k in range(0,len(eps))]\n",
    "    except:\n",
    "        print('oops! couldnt clean it')\n",
    "        return None\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create dataset of episodes\n",
    "\n",
    "\n",
    "# First, let us load the podcasts dataframe we created in the last section.\n",
    "floc = '/Users/mohsenghassemi/Desktop/Job_Material/Insight/Project/summaries6/'\n",
    "associator = '1580326866'\n",
    "fname = 'popular_podcasts_preprocessed' +'_'+ associator + '.pkl'\n",
    "with open(floc+fname,'rb') as fid:\n",
    "         podcasts_df=pickle.load(fid)\n",
    "        \n",
    "# Next, parse and clean the episode descriptions to prepare them for BERT (keep stopwords and punctuations).\n",
    "podcasts_df['episodes_descriptions_unprocessed'] = podcasts_df.apply(lambda x:get_unprocessed_eps(x),axis=1)\n",
    "podcasts_df['episodes_descriptions_processed'] = podcasts_df['episodes_descriptions_unprocessed'].apply(lambda x:get_processed_eps(x))\n",
    "\n",
    "\n",
    "# Here, we create a dataframe where each row corresponds to one episode.\n",
    "temp=podcasts_df.merge(podcasts_df.episodes_descriptions_processed.apply(pd.Series), left_index = True, right_index = True)\n",
    "temp.drop(['episodes_descriptions_unprocesses','podcasts_eps_descriptions'], axis=1, inplace=True)\n",
    "episodes_df=temp.melt(id_vars = ['feedUrl', 'episodes_descriptions_processed'], value_name = 'episodes_descriptions')\n",
    "episodes_df.dropna(inplace=True)\n",
    "episodes_df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "with open('processed_BERT','wb') as fid:\n",
    "         pickle.dump(podcasts_df,fid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In sentence-BERT, we find embedding for every sentence in the episode descriptions, therefore we split episode desctiptions into their constructing sentences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Split each episode description to sentences to prepare it to be used for Senetence-BERT'''\n",
    "with open('processed_BERT','rb') as fid:\n",
    "         episodes_df=pickle.load(fid)\n",
    "        \n",
    "episodes_df['episodes_descriptions_sentences'] = episodes_df['episodes_descriptions'].apply(lambda x: ['. '.join(x.split('. ')[0:4])])\n",
    "episodes_df.drop(['episodes_descriptions_processed','episodes_descriptions'],axis=1,inplace=True)\n",
    "with open('processed_BERT_first4sentences','wb') as fid:\n",
    "         pickle.dump(episodes_df,fid)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
